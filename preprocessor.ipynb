{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.file = ''\n",
    "        self.keys,self.replace,self.dicts = self.load_keys_and_replacements()\n",
    "\n",
    "    def load_keys_and_replacements(self):\n",
    "        keys = list(map(lambda x:x.replace('\\n',''), self.read_txt('keys/keywords.txt')))\n",
    "        replace = list(map(lambda x:x.replace('\\n',''), self.read_txt('keys/replace.txt')))\n",
    "        dicts = {}\n",
    "        for line in replace:\n",
    "            x,y = line.split('==')[0].replace(' ',''), line.split('==')[1].replace(' ','')\n",
    "            dicts[x] = y\n",
    "        return keys,replace, dicts\n",
    "        \n",
    "    def read_txt(self,path):\n",
    "        f = open(path)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        return lines\n",
    "    \n",
    "    def tokenize(self,txt):\n",
    "        return txt.split(' ')\n",
    "    \n",
    "    def replace_symbol(self,word):\n",
    "        wordlist = []\n",
    "        try:\n",
    "            type(int(word))\n",
    "            wordlist.append('INT')\n",
    "        except:\n",
    "            word = list(word)\n",
    "            for i,l in enumerate(word):\n",
    "                try:\n",
    "                    repl = self.dicts[l]\n",
    "                except:\n",
    "                    repl = None\n",
    "                    \n",
    "                if repl != None:\n",
    "                    l = \" \"+repl+' '\n",
    "                wordlist.append(l)\n",
    "        \n",
    "        return \" \".join(\"\".join(wordlist).strip().split(\"  \"))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def Converter(self,query):\n",
    "        query = query.lower()\n",
    "        bb = list(map(self.replace_symbol, query.split(' ')))\n",
    "        bb = ' '.join(bb)\n",
    "        tokenized = ' '.join(list(map(self.replace_symbol,bb.split(' '))))\n",
    "        split = tokenized.split(' ')\n",
    "        for i,word in enumerate(tokenized.split(' ')):\n",
    "            if word.upper() in self.keys:\n",
    "                split[i] = word.upper()\n",
    "        return \" \".join(split)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.preprocess = Preprocess()\n",
    "        self.all_vocabs = []\n",
    "    def open_file(self,filename,label,safe=False,limit=0):\n",
    "        f = open(filename,'r')\n",
    "        if limit == 0:\n",
    "            txt = f.readlines()\n",
    "        else:\n",
    "            txt = f.readlines()[:limit]\n",
    "        f.close()\n",
    "        text = []\n",
    "        for sent in txt:\n",
    "            if len(sent) > 0:\n",
    "                sent_list = []\n",
    "                sent = sent.replace(\"\\n\",'').split(\" \")\n",
    "                for word in sent:\n",
    "                    if len(word)> 0:\n",
    "                        sent_list.append(word)\n",
    "                if safe:\n",
    "                    text.append(\" \".join(sent_list))\n",
    "                else:\n",
    "                    text.append(self.preprocess.Converter(\" \".join(sent_list)))\n",
    "        dict = {'text':text,'label':[label for i in range(len(text))]}\n",
    "        data = pd.DataFrame(dict)\n",
    "        self.all_vocabs.append([sent for sent in data['text']])\n",
    "        return data\n",
    "    \n",
    "    def save_multiple(self,paths_list):\n",
    "        all_pds = []\n",
    "        for file in paths_list:\n",
    "            data = self.open_file(file,file)\n",
    "            all_pds.append(data)\n",
    "        return all_pds\n",
    "    \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    def __init__(self ):\n",
    "        self.vocab = set()\n",
    "        self.labels = set()\n",
    "        self.index_2_token = {}\n",
    "        self.token_2_index = {}\n",
    "        self.labels_2_index = {}\n",
    "        self.index_2_labels = {}\n",
    "        \n",
    "    def build_vocab(self,csv_file):\n",
    "        df = csv_file\n",
    "        \n",
    "        #lets build the vocab\n",
    "        for data in df.iterrows():\n",
    "            text, label = data[1]['text'], data[1]['label']\n",
    "            self.add_token(text)\n",
    "            self.labels.add(label)\n",
    "    \n",
    "            \n",
    "    def tokenize(self,txt):\n",
    "        return txt.split(' ')\n",
    "    \n",
    "    def add_token(self,txt):\n",
    "        tokenized = self.tokenize(txt)\n",
    "        for tok in tokenized:\n",
    "            self.vocab.add(tok)\n",
    "            \n",
    "    def get_vocab(self):\n",
    "        return list(self.vocab)\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return {lbl:i for i,lbl in self.labels}\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Build_Dataset():\n",
    "    def __init_(self, pds):\n",
    "        self.all_pds= pd.concat(pds)\n",
    "        self.vectorizer = Vecotrizer()\n",
    "        \n",
    "        for pd in pds:\n",
    "            self.vectorizer.build_vocab(pd)\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LT script GT alert LPRN INT RPRN LT SLSH script'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Preprocess().Converter('<script>alert(1)</script')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Build_Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
