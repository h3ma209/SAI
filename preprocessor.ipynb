{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.file = ''\n",
    "        self.keys,self.replace,self.dicts = self.load_keys_and_replacements()\n",
    "\n",
    "    def load_keys_and_replacements(self):\n",
    "        keys = list(map(lambda x:x.replace('\\n',''), self.read_txt('keys/keywords.txt')))\n",
    "        replace = list(map(lambda x:x.replace('\\n',''), self.read_txt('keys/replace.txt')))\n",
    "        dicts = {}\n",
    "        for line in replace:\n",
    "            x,y = line.split('==')[0].replace(' ',''), line.split('==')[1].replace(' ','')\n",
    "            dicts[x] = y\n",
    "        return keys,replace, dicts\n",
    "        \n",
    "    def read_txt(self,path):\n",
    "        f = open(path)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        return lines\n",
    "    \n",
    "    def tokenize(self,txt):\n",
    "        return txt.split(' ')\n",
    "    \n",
    "    def replace_symbol(self,word):\n",
    "        wordlist = []\n",
    "        try:\n",
    "            type(int(word))\n",
    "            wordlist.append('INT')\n",
    "        except:\n",
    "            word = list(word)\n",
    "            for i,l in enumerate(word):\n",
    "                try:\n",
    "                    repl = self.dicts[l]\n",
    "                except:\n",
    "                    repl = None\n",
    "                    \n",
    "                if repl != None:\n",
    "                    l = \" \"+repl+' '\n",
    "                wordlist.append(l)\n",
    "        \n",
    "        return \" \".join(\"\".join(wordlist).strip().split(\"  \"))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def Converter(self,query):\n",
    "        query = query.lower()\n",
    "        bb = list(map(self.replace_symbol, query.split(' ')))\n",
    "        bb = ' '.join(bb)\n",
    "        tokenized = ' '.join(list(map(self.replace_symbol,bb.split(' '))))\n",
    "        split = tokenized.split(' ')\n",
    "        for i,word in enumerate(tokenized.split(' ')):\n",
    "            if word.upper() in self.keys:\n",
    "                split[i] = word.upper()\n",
    "        return \" \".join(split)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.preprocess = Preprocess()\n",
    "        self.all_vocabs = []\n",
    "    def open_file(self,filename,label,safe=False,limit=0):\n",
    "        f = open(filename,'r')\n",
    "        if limit == 0:\n",
    "            txt = f.readlines()\n",
    "        else:\n",
    "            txt = f.readlines()[:limit]\n",
    "        f.close()\n",
    "        text = []\n",
    "        for sent in txt:\n",
    "            if len(sent) > 0:\n",
    "                sent_list = []\n",
    "                sent = sent.replace(\"\\n\",'').split(\" \")\n",
    "                for word in sent:\n",
    "                    if len(word)> 0:\n",
    "                        sent_list.append(word)\n",
    "                if safe:\n",
    "                    text.append(\" \".join(sent_list))\n",
    "                else:\n",
    "                    text.append(self.preprocess.Converter(\" \".join(sent_list)))\n",
    "        dict = {'text':text,'label':[label for i in range(len(text))]}\n",
    "        data = pd.DataFrame(dict)\n",
    "        self.all_vocabs.append([sent for sent in data['text']])\n",
    "        return data\n",
    "    \n",
    "    def save_multiple(self,paths_list):\n",
    "        all_pds = []\n",
    "        for file in paths_list:\n",
    "            data = self.open_file(file,file)\n",
    "            all_pds.append(data)\n",
    "        return all_pds\n",
    "    \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LT script GT alert LPRN INT RPRN LT SLSH script'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Preprocess().Converter('<script>alert(1)</script')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata = MakeDataset().save_multiple(['XSS-Payloads/payload.txt','safe.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    def __init__(self ):\n",
    "        self.vocab = set()\n",
    "        self.labels = set()\n",
    "        self.index_2_token = {}\n",
    "        self.token_2_index = {}\n",
    "        self.labels_2_index = {}\n",
    "        self.index_2_labels = {}\n",
    "        \n",
    "    def build_vocab(self,csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        #lets build the vocab\n",
    "        for data in df.iterrows():\n",
    "            text, label = data[1]['text'], data[1]['label']\n",
    "            self.add_token(text)\n",
    "            self.labels.add(label)\n",
    "            \n",
    "    def tokenize(self,txt):\n",
    "        return txt.split(' ')\n",
    "    \n",
    "    def add_token(self,txt):\n",
    "        tokenized = self.tokenize(txt)\n",
    "        for tok in tokenized:\n",
    "            self.vocab.add(tok)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "xss = rdata[0]\n",
    "safe = rdata[1][:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_file = 'csv_files/safe_xss.csv'\n",
    "xss.to_csv(save_to_file)\n",
    "safe.to_csv(save_to_file)\n",
    "del xss, safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XSS-Payloads/payload.txt\n"
     ]
    }
   ],
   "source": [
    "for i in xss.iterrows():\n",
    "    print(i[1]['label'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = Vectorizer()\n",
    "vec.build_vocab('csv_files/safe_xss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'<script>' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-eeab4e492b04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<script>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: '<script>' is not in list"
     ]
    }
   ],
   "source": [
    "list(vec.vocab).index('<script>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
