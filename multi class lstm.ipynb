{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torchtext import data    \n",
    "from libs import sql_tokenizer\n",
    "import libs\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reproducing same results\n",
    "SEED = 2019\n",
    "\n",
    "#Torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "#Cuda algorithms\n",
    "torch.backends.cudnn.deterministic = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [(None, None), ('text',TEXT),('label', LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['LT', 'script', 'GT', 'alert', 'LPRN', 'INT', 'RPRN', 'SMCLN', 'LT', 'SLSH', 'script', 'GT'], 'label': 'xss'}\n",
      "19 <blank> xss\n",
      "20 <blank> xss\n",
      "22 <blank> xss\n",
      "24 <blank> xss\n",
      "26 <blank> xss\n",
      "28 <blank> xss\n",
      "30 <blank> xss\n",
      "32 <blank> xss\n",
      "34 <blank> xss\n",
      "36 <blank> xss\n",
      "38 <blank> xss\n",
      "40 <blank> xss\n",
      "42 <blank> xss\n",
      "44 <blank> xss\n",
      "46 <blank> xss\n",
      "48 <blank> xss\n",
      "50 <blank> xss\n",
      "52 <blank> xss\n",
      "55 <blank> xss\n",
      "57 <blank> xss\n",
      "61 <blank> xss\n",
      "63 <blank> xss\n",
      "65 <blank> xss\n",
      "67 <blank> xss\n",
      "69 <blank> xss\n",
      "71 <blank> xss\n",
      "73 <blank> xss\n",
      "75 <blank> xss\n",
      "77 <blank> xss\n",
      "79 <blank> xss\n",
      "81 <blank> xss\n",
      "83 <blank> xss\n",
      "85 <blank> xss\n",
      "87 <blank> xss\n",
      "89 <blank> xss\n",
      "91 <blank> xss\n",
      "93 <blank> xss\n",
      "95 <blank> xss\n",
      "97 <blank> xss\n",
      "99 <blank> xss\n",
      "101 <blank> xss\n",
      "103 <blank> xss\n",
      "105 <blank> xss\n",
      "107 <blank> xss\n",
      "109 <blank> xss\n",
      "111 <blank> xss\n",
      "113 <blank> xss\n",
      "115 <blank> xss\n",
      "117 <blank> xss\n",
      "119 <blank> xss\n",
      "121 <blank> xss\n",
      "123 <blank> xss\n",
      "125 <blank> xss\n",
      "127 <blank> xss\n",
      "129 <blank> xss\n",
      "131 <blank> xss\n",
      "133 <blank> xss\n",
      "135 <blank> xss\n",
      "137 <blank> xss\n",
      "140 <blank> xss\n",
      "142 <blank> xss\n",
      "144 <blank> xss\n",
      "146 <blank> xss\n",
      "148 <blank> xss\n",
      "150 <blank> xss\n",
      "152 <blank> xss\n",
      "154 <blank> xss\n",
      "156 <blank> xss\n",
      "158 <blank> xss\n",
      "161 <blank> xss\n",
      "164 <blank> xss\n",
      "166 <blank> xss\n",
      "168 <blank> xss\n",
      "170 <blank> xss\n",
      "173 <blank> xss\n",
      "175 <blank> xss\n",
      "177 <blank> xss\n",
      "179 <blank> xss\n",
      "181 <blank> xss\n",
      "183 <blank> xss\n",
      "186 <blank> xss\n",
      "188 <blank> xss\n",
      "190 <blank> xss\n",
      "192 <blank> xss\n",
      "195 <blank> xss\n",
      "197 <blank> xss\n",
      "199 <blank> xss\n",
      "201 <blank> xss\n",
      "203 <blank> xss\n",
      "205 <blank> xss\n",
      "207 <blank> xss\n",
      "209 <blank> xss\n",
      "764 <blank> sql\n",
      "840 <blank> sql\n"
     ]
    }
   ],
   "source": [
    "training_data=data.TabularDataset(path = 'csv_files/xss_safe_sql.csv',format = 'csv',fields = fields,skip_header = True)\n",
    "\n",
    "#print preprocessed text\n",
    "print(vars(training_data.examples[0]))\n",
    "for i,dt in enumerate(training_data):\n",
    "    if(len(dt.text) <= 0):\n",
    "        training_data[i].text = \"<blank>\"\n",
    "        print(i,training_data[i].text,training_data[i].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 1726\n",
      "Size of LABEL vocabulary: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x000001F89A6FA860>>, {'<unk>': 0, '<pad>': 1, 'DOT': 2, 'the': 3, 'INT': 4, 'RPRN': 5, 'CMMA': 6, 'A': 7, 'SMCLN': 8, 'AND': 9, 'BITAND': 10, 'LPRN': 11, 'SQUT': 12, 'OF': 13, 'NULL': 14, 'TO': 15, 'SLSH': 16, 'IS': 17, 'i': 18, 'br': 19, 'it': 20, 'PRCNT': 21, 'EQ': 22, 'IN': 23, 'DQUT': 24, 'that': 25, 'this': 26, 'MINUS': 27, 'HASH': 28, 'GT': 29, 'LT': 30, 's': 31, 'movie': 32, 'CLN': 33, 'was': 34, 'BSLSH': 35, 'AS': 36, 'but': 37, 'WITH': 38, 'film': 39, 'ON': 40, 'FOR': 41, 'script': 42, 'T': 43, 'NOT': 44, 'FROM': 45, 'you': 46, 'alert': 47, 'STAR': 48, 'OR': 49, 'IF': 50, 'ARE': 51, 'ALL': 52, 'be': 53, 'tab': 54, 'they': 55, 'have': 56, 'one': 57, 'gt': 58, 'lt': 59, 'an': 60, 'his': 61, 'xss': 62, 'he': 63, 'SELECT': 64, 'who': 65, 'benchmark': 66, 'md5': 67, 'AT': 68, 'so': 69, 'PLUS': 70, 'BY': 71, 'LIKE': 72, 'story': 73, 'src': 74, 'OUT': 75, 'just': 76, 'good': 77, 'titanic': 78, 'can': 79, 'about': 80, 'were': 81, 'there': 82, 'has': 83, 'WHEN': 84, 'TIME': 85, 'MORE': 86, 'NO': 87, 'my': 88, 'her': 89, 'even': 90, 'what': 91, 'ONLY': 92, 'she': 93, 'because': 94, 'very': 95, 'b': 96, 'would': 97, '20delay': 98, '20waitfor': 99, 'SOME': 100, 'really': 101, 'WHERE': 102, 'l': 103, 'n': 104, 'see': 105, 'well': 106, '<': 107, '>': 108, 'FIRST': 109, 'a': 110, 'had': 111, 'k': 112, 'will': 113, 'their': 114, 'which': 115, 'people': 116, 'its': 117, 'don': 118, 'much': 119, 'THEN': 120, 'ATR': 121, 'STYLE': 122, 'me': 123, 'DISTINCT': 124, 'movies': 125, 'javascript': 126, 'up': 127, 'CHARACTERS': 128, 'INTO': 129, 'LIMIT': 130, 'such': 131, 'been': 132, 'could': 133, 'great': 134, 'http': 135, 'love': 136, 'DO': 137, 'img': 138, 'AFTER': 139, 'scene': 140, 'too': 141, 'bad': 142, 'how': 143, 'rose': 144, 'watch': 145, 'many': 146, 'than': 147, 'think': 148, 'GET': 149, 'make': 150, 'scenes': 151, 'ANY': 152, 'did': 153, 'most': 154, 'never': 155, 'them': 156, 'OLD': 157, 'made': 158, 'other': 159, 'ACTION': 160, 'GO': 161, 'should': 162, 'being': 163, 'we': 164, 'best': 165, 'director': 166, 'ship': 167, 'EXCLM': 168, 'films': 169, 'little': 170, 'why': 171, 'x': 172, 'delay': 173, 'life': 174, 'waitfor': 175, 'way': 176, 'OVER': 177, 'plot': 178, 'acting': 179, 'him': 180, 'two': 181, 'your': 182, 'actors': 183, 'does': 184, 'jack': 185, 'seen': 186, 'better': 187, 'ever': 188, 'fromcharcode': 189, 'get_host_address': 190, 'string': 191, 'utl_inaddr': 192, 'ALSO': 193, 'didn': 194, 'know': 195, 'these': 196, 'NEW': 197, 'now': 198, 'tashan': 199, 'EVERY': 200, 'WHILE': 201, 'still': 202, 'years': 203, 'CHARACTER': 204, 'M': 205, 'ckers': 206, 'give': 207, 'ha': 208, 'man': 209, 'music': 210, 'org': 211, 'say': 212, 'watching': 213, 'END': 214, 'OFF': 215, 'REAL': 216, 'down': 217, 'horror': 218, 'rownum': 219, 'those': 220, 'NOTHING': 221, 'back': 222, 'james': 223, 'something': 224, 'through': 225, 'BOTH': 226, 'LCBR': 227, 'VERSION': 228, 'got': 229, 'maybe': 230, 'sys': 231, 'thought': 232, 've': 233, 'LAST': 234, 'YEAR': 235, 'akshay': 236, 'kapoor': 237, 'kareena': 238, 'look': 239, 'makes': 240, 'newline': 241, 'part': 242, 'since': 243, 'though': 244, 'woman': 245, '”': 246, 'INSTEAD': 247, 'WORK': 248, 'fact': 249, 'going': 250, 'here': 251, 'iframe': 252, 'superman': 253, 'BETWEEN': 254, 'CLASS': 255, 'FAMILY': 256, 'RCBR': 257, 'TEXT': 258, 'actually': 259, 'another': 260, 'cameron': 261, 'doesn': 262, 'everything': 263, 'few': 264, 'find': 265, 'however': 266, 'lot': 267, 'making': 268, 'night': 269, 'said': 270, 'things': 271, 'want': 272, 'young': 273, 'QSTN': 274, 'SET': 275, 'WITHOUT': 276, 'done': 277, 'girl': 278, 'may': 279, 'money': 280, 'must': 281, 're': 282, 'same': 283, 'BEFORE': 284, 'BITOR': 285, 'COUNT': 286, 'HAVING': 287, 'again': 288, 'am': 289, 'audience': 290, 'heart': 291, 'interesting': 292, 'js': 293, 'looks': 294, 'saif': 295, 'thing': 296, 'almost': 297, 'anything': 298, 'big': 299, 'far': 300, 'help': 301, 'job': 302, 'line': 303, 'onerror': 304, 'point': 305, 'probably': 306, 'svg': 307, 'wonderful': 308, 'DATA': 309, 'EXEC': 310, 'PASSWORD': 311, 'READ': 312, 'SHOW': 313, 'amazing': 314, 'anil': 315, 'beautiful': 316, 'body': 317, 'come': 318, 'effects': 319, 'excellent': 320, 'foo': 321, 'house': 322, 'href': 323, 'mother': 324, 'performances': 325, 'rich': 326, 'times': 327, 'trying': 328, 'wasn': 329, 'women': 330, 'world': 331, 'worst': 332, 'RIGHT': 333, 'TYPE': 334, '_': 335, 'especially': 336, 'everyone': 337, 'grease': 338, 'html': 339, 'less': 340, 'll': 341, 'original': 342, 'pretty': 343, 'saw': 344, 'take': 345, 'top': 346, 'us': 347, 'username': 348, 'worth': 349, '’': 350, '20if': 351, 'DLLR': 352, 'DOCUMENT': 353, 'LONG': 354, 'ROLE': 355, 'art': 356, 'away': 357, 'boy': 358, 'came': 359, 'cut': 360, 'definitely': 361, 'english': 362, 'enough': 363, 'high': 364, 'japanese': 365, 'kumar': 366, 'masks': 367, 'master': 368, 'mind': 369, 'minutes': 370, 'onload': 371, 'performance': 372, 'sense': 373, 'song': 374, 'special': 375, 'BIT': 376, 'DAY': 377, 'ELSE': 378, 'NAME': 379, 'actor': 380, 'boring': 381, 'com': 382, 'completely': 383, 'dvd': 384, 'ending': 385, 'fun': 386, 'goes': 387, 'lester': 388, 'liked': 389, 'lost': 390, 'overall': 391, 'played': 392, 'quite': 393, 'seeing': 394, 'shown': 395, 'songs': 396, 'street': 397, 'used': 398, 'wang': 399, 'whole': 400, 'williams': 401, 'winslet': 402, 'won': 403, 'CAST': 404, 'CONTENT': 405, 'UNTIL': 406, 'anyone': 407, 'background': 408, 'cal': 409, 'different': 410, 'donner': 411, 'either': 412, 'fans': 413, 'gets': 414, 'idea': 415, 'king': 416, 'might': 417, 'moving': 418, 'reason': 419, 'romance': 420, 'stars': 421, 'takes': 422, 'terrible': 423, 'together': 424, 'url': 425, 'DIV': 426, 'FOUND': 427, 'LEAST': 428, 'SECOND': 429, 'UNION': 430, 'YES': 431, 'although': 432, 'colon': 433, 'face': 434, 'feel': 435, 'finally': 436, 'granted_role': 437, 'isn': 438, 'male': 439, 'perfect': 440, 'problem': 441, 'seem': 442, 'star': 443, 'supposed': 444, 'tv': 445, 'u0061': 446, 'voyage': 447, 'INPUT': 448, 'LEAVE': 449, 'NEXT': 450, 'RQUT': 451, 'believe': 452, 'comes': 453, 'd': 454, 'dicaprio': 455, 'disappointed': 456, 'during': 457, 'enjoy': 458, 'entire': 459, 'favorite': 460, 'girls': 461, 'gone': 462, 'himself': 463, 'home': 464, 'kind': 465, 'let': 466, 'mean': 467, 'ocean': 468, 'once': 469, 'own': 470, 'put': 471, 'rent': 472, 'school': 473, 'screen': 474, 'short': 475, 'simply': 476, 'situation': 477, 'slow': 478, 'someone': 479, 'thinking': 480, '‘': 481, 'ALWAYS': 482, 'INCLUDING': 483, 'LEFT': 484, 'LSQBR': 485, 'OBJECT': 486, 'RELEASE': 487, 'START': 488, 'TRUE': 489, 'USER': 490, 'WRITE': 491, 'along': 492, 'china': 493, 'couple': 494, 'directed': 495, 'equiv': 496, 'expect': 497, 'expected': 498, 'footage': 499, 'getting': 500, 'guess': 501, 'guy': 502, 'history': 503, 'iceberg': 504, 'kate': 505, 'main': 506, 'meta': 507, 'prompt': 508, 'rather': 509, 'recommend': 510, 'rest': 511, 'serious': 512, 'stupid': 513, 'sure': 514, 'turn': 515, 'visual': 516, 'white': 517, 'x3c': 518, 'yet': 519, 'CALLED': 520, 'COLUMN_NAME': 521, 'FULL': 522, 'REMOVE': 523, 'RSQBR': 524, 'TABLE_NAME': 525, 'XML': 526, 'becomes': 527, 'beginning': 528, 'believable': 529, 'bollywood': 530, 'cookie': 531, 'course': 532, 'disaster': 533, 'doggie': 534, 'easily': 535, 'enjoyable': 536, 'experience': 537, 'eye': 538, 'felt': 539, 'five': 540, 'form': 541, 'given': 542, 'god': 543, 'hand': 544, 'happened': 545, 'hard': 546, 'hate': 547, 'hit': 548, 'hollywood': 549, 'human': 550, 'itself': 551, 'kids': 552, 'later': 553, 'leonardo': 554, 'looked': 555, 'looking': 556, 'material': 557, 'myfile': 558, 'nice': 559, 'often': 560, 'oh': 561, 'opera': 562, 'our': 563, 'pass': 564, 'person': 565, 'piece': 566, 'plays': 567, 'problems': 568, 'production': 569, 'remember': 570, 'roles': 571, 'seems': 572, 'strong': 573, 'stuff': 574, 'tale': 575, 'talent': 576, 'tell': 577, 'three': 578, 'timberlake': 579, 'truly': 580, 'watched': 581, 'went': 582, 'CASE': 583, 'EACH': 584, 'NONE': 585, 'NUMBER': 586, 'POWER': 587, 'SIMPLE': 588, 'TABLE': 589, 'UNDER': 590, 'absolutely': 591, 'across': 592, 'around': 593, 'attempt': 594, 'awful': 595, 'based': 596, 'become': 597, 'black': 598, 'bolt': 599, 'camera': 600, 'child': 601, 'children': 602, 'chinese': 603, 'collette': 604, 'complete': 605, 'confirm': 606, 'cool': 607, 'crap': 608, 'current_setting': 609, 'dark': 610, 'daughter': 611, 'days': 612, 'death': 613, 'despite': 614, 'dialogue': 615, 'direction': 616, 'directors': 617, 'drama': 618, 'emotional': 619, 'episode': 620, 'extremely': 621, 'fan': 622, 'feeling': 623, 'forget': 624, 'genre': 625, 'gives': 626, 'grantee': 627, 'half': 628, 'hope': 629, 'image': 630, 'interest': 631, 'jimmy': 632, 'kill': 633, 'listener': 634, 'live': 635, 'lommel': 636, 'loved': 637, 'lugia': 638, 'matter': 639, 'meet': 640, 'men': 641, 'not': 642, 'performer': 643, 'place': 644, 'pokmon': 645, 'project': 646, 'recommended': 647, 'score': 648, 'scr': 649, 'seemed': 650, 'shot': 651, 'sinking': 652, 'talk': 653, 'tells': 654, 'third': 655, 'told': 656, 'understand': 657, 'var': 658, 'wants': 659, 'waste': 660, 'written': 661, 'yash': 662, '20or': 663, 'CALL': 664, 'COMMENTS': 665, 'CREATE': 666, 'GROUP': 667, 'LOCATION': 668, 'REFRESH': 669, 'accurate': 670, 'acted': 671, 'actual': 672, 'addition': 673, 'admit': 674, 'ali': 675, 'allen': 676, 'already': 677, 'american': 678, 'award': 679, 'badly': 680, 'beauty': 681, 'became': 682, 'brilliant': 683, 'car': 684, 'care': 685, 'caulfield': 686, 'cinematography': 687, 'cry': 688, 'culture': 689, 'decided': 690, 'die': 691, 'doing': 692, 'doubt': 693, 'early': 694, 'earth': 695, 'embed': 696, 'epic': 697, 'etc': 698, 'fall': 699, 'famous': 700, 'fast': 701, 'fine': 702, 'gave': 703, 'google': 704, 'head': 705, 'heard': 706, 'highly': 707, 'horrible': 708, 'hot': 709, 'ii': 710, 'justin': 711, 'kept': 712, 'khan': 713, 'lives': 714, 'major': 715, 'managed': 716, 'mention': 717, 'molly': 718, 'mr': 719, 'murder': 720, 'needed': 721, 'ok': 722, 'onmouseover': 723, 'passengers': 724, 'pay': 725, 'perhaps': 726, 'picture': 727, 'pikachu': 728, 'q': 729, 'richard': 730, 'ridiculous': 731, 'robin': 732, 'series': 733, 'side': 734, 'social': 735, 'sorry': 736, 'speaking': 737, 'started': 738, 'stories': 739, 'straight': 740, 'suspense': 741, 'taking': 742, 'theater': 743, 'title': 744, 'tobacco': 745, 'today': 746, 'toni': 747, 'tried': 748, 'try': 749, 'turned': 750, 'turns': 751, 'u': 752, 'video': 753, 'viewer': 754, 'vijay': 755, 'whatever': 756, 'woody': 757, 'works': 758, 'worse': 759, 'wreck': 760, 'www': 761, 'x000003c': 762, 'yashraj': 763, 'C': 764, 'COPY': 765, 'DECLARE': 766, 'EXCEPT': 767, 'FOREIGN': 768, 'MATCH': 769, 'OPEN': 770, 'SEARCH': 771, 'UPPER': 772, 'USE': 773, 'VIEW': 774, 'above': 775, 'acharya': 776, 'ad4': 777, 'adult': 778, 'against': 779, 'alone': 780, 'anyway': 781, 'ash': 782, 'aside': 783, 'atlantic': 784, 'bergman': 785, 'bhaiyyaji': 786, 'bikini': 787, 'break': 788, 'brooks': 789, 'brought': 790, 'budget': 791, 'captain': 792, 'classic': 793, 'comedy': 794, 'cultural': 795, 'dba_role_privs': 796, 'deeply': 797, 'dialog': 798, 'drawing': 799, 'elements': 800, 'emotions': 801, 'enjoyed': 802, 'entertainment': 803, 'events': 804, 'exactly': 805, 'expecting': 806, 'eyes': 807, 'fictional': 808, 'finds': 809, 'freeman': 810, 'friend': 811, 'friendship': 812, 'funny': 813, 'future': 814, 'ghost': 815, 'giving': 816, 'haven': 817, 'heir': 818, 'hell': 819, 'herself': 820, 'historical': 821, 'humor': 822, 'impressive': 823, 'indian': 824, 'intelligent': 825, 'ipt': 826, 'issue': 827, 'john': 828, 'kid': 829, 'knew': 830, 'krishna': 831, 'learn': 832, 'leaves': 833, 'loginuser': 834, 'lol': 835, 'lpar': 836, 'm': 837, 'means': 838, 'members': 839, 'memorable': 840, 'mess': 841, 'moments': 842, 'needs': 843, 'neither': 844, 'officer': 845, 'oscar': 846, 'outstanding': 847, 'pacing': 848, 'parts': 849, 'past': 850, 'pieces': 851, 'play': 852, 'plus': 853, 'points': 854, 'police': 855, 'poor': 856, 'possible': 857, 'predictable': 858, 'present': 859, 'raj': 860, 'reading': 861, 'romantic': 862, 'rpar': 863, 'sad': 864, 'save': 865, 'seat': 866, 'sees': 867, 'shame': 868, 'showing': 869, 'shows': 870, 'sort': 871, 'soundtrack': 872, 'stop': 873, 'sunk': 874, 'taken': 875, 'teens': 876, 'ten': 877, 'theme': 878, 'throughout': 879, 'towelhead': 880, 'tragic': 881, 'unexpected': 882, 'unfortunately': 883, 'usual': 884, 'wife': 885, 'wrong': 886, 'x03c': 887, 'zero': 888, 'BASE64': 889, 'CHAR': 890, 'CLOSE': 891, 'CURRENT': 892, 'EXPRESSION': 893, 'GENERAL': 894, 'HOUR': 895, 'MOVE': 896, 'TLDE': 897, 'USING': 898, 'VALUE': 899, 'VALUES': 900, 'VARCHAR': 901, 'able': 902, 'act': 903, 'actress': 904, 'added': 905, 'adventure': 906, 'adw': 907, 'agree': 908, 'all_tab_columns': 909, 'all_tables': 910, 'all_users': 911, 'annoying': 912, 'apart': 913, 'apparently': 914, 'appears': 915, 'april': 916, 'ask': 917, 'assembly': 918, 'audiences': 919, 'average': 920, 'basic': 921, 'behind': 922, 'birds': 923, 'blame': 924, 'bobby': 925, 'book': 926, 'bottom': 927, 'brown': 928, 'change': 929, 'cheesy': 930, 'chopra': 931, 'code': 932, 'color': 933, 'compelling': 934, 'complaint': 935, 'country': 936, 'created': 937, 'creepy': 938, 'crew': 939, 'dance': 940, 'dead': 941, 'deserves': 942, 'desperate': 943, 'editing': 944, 'effective': 945, 'entertaining': 946, 'ex': 947, 'experiences': 948, 'fashion': 949, 'fi': 950, 'filled': 951, 'filmed': 952, 'four': 953, 'hours': 954, 'husband': 955, 'impossible': 956, 'killed': 957, 'killer': 958, 'knows': 959, 'lesbian': 960, 'liang': 961, 'lifetime': 962, 'lois': 963, 'longer': 964, 'low': 965, 'magic': 966, 'maiden': 967, 'makers': 968, 'masterpiece': 969, 'masters': 970, 'michael': 971, 'michelle': 972, 'modern': 973, 'moment': 974, 'musical': 975, 'nature': 976, 'need': 977, 'noon': 978, 'obviously': 979, 'opening': 980, 'parent': 981, 'peter': 982, 'plain': 983, 'playing': 984, 'please': 985, 'pooja': 986, 'port': 987, 'print': 988, 'quality': 989, 'r': 990, 'realistic': 991, 'relationship': 992, 'review': 993, 'rogers': 994, 'room': 995, 'sadly': 996, 'sci': 997, 'screwfly': 998, 'scriptlet': 999, 'sequel': 1000, 'sex': 1001, 'sexuality': 1002, 'shooting': 1003, 'sight': 1004, 'silly': 1005, 'smith': 1006, 'somewhat': 1007, 'son': 1008, 'soon': 1009, 'sound': 1010, 'streets': 1011, 'student': 1012, 'sucked': 1013, 'supporting': 1014, 'surprised': 1015, 'sysibm': 1016, 'talking': 1017, 'team': 1018, 'tears': 1019, 'television': 1020, 'thanks': 1021, 'thriller': 1022, 'took': 1023, 'tradition': 1024, 'traditional': 1025, 'trash': 1026, 'u0074': 1027, 'ultimately': 1028, 'utter': 1029, 'voice': 1030, 'wearing': 1031, 'wish': 1032, 'writer': 1033, 'x003c': 1034, 'xp_cmdshell': 1035, 'york': 1036, '2a': 1037, '3e': 1038, 'BEGIN': 1039, 'CONTINUE': 1040, 'EVENT': 1041, 'FORWARD': 1042, 'HOLD': 1043, 'INSERT': 1044, 'LARGE': 1045, 'LEAD': 1046, 'LOWER': 1047, 'NAMES': 1048, 'P': 1049, 'RANGE': 1050, 'SETS': 1051, 'SIMILAR': 1052, 'SUBSTRING': 1053, 'UNIQUE': 1054, 'WITHIN': 1055, 'academy': 1056, 'actresses': 1057, 'ago': 1058, 'ahead': 1059, 'alright': 1060, 'appear': 1061, 'artist': 1062, 'artistic': 1063, 'ascript': 1064, 'atmosphere': 1065, 'attempts': 1066, 'attention': 1067, 'autofocus': 1068, 'bachchan': 1069, 'becoming': 1070, 'begins': 1071, 'ben': 1072, 'bet': 1073, 'biggest': 1074, 'billy': 1075, 'bird': 1076, 'boss': 1077, 'bother': 1078, 'bring': 1079, 'brings': 1080, 'bromwell': 1081, 'bukater': 1082, 'career': 1083, 'cheap': 1084, 'childhood': 1085, 'clever': 1086, 'coming': 1087, 'complain': 1088, 'confused': 1089, 'considering': 1090, 'cop': 1091, 'costumes': 1092, 'couldn': 1093, 'creating': 1094, 'css': 1095, 'damn': 1096, 'dancing': 1097, 'dawson': 1098, 'decent': 1099, 'decide': 1100, 'deck': 1101, 'delivery': 1102, 'depicted': 1103, 'development': 1104, 'dewitt': 1105, 'dialogs': 1106, 'diamond': 1107, 'died': 1108, 'difference': 1109, 'dil': 1110, 'direct': 1111, 'directing': 1112, 'dirty': 1113, 'disbelief': 1114, 'dollars': 1115, 'dramatic': 1116, 'echo': 1117, 'edge': 1118, 'emotion': 1119, 'engagement': 1120, 'european': 1121, 'failed': 1122, 'fantastic': 1123, 'feelings': 1124, 'feels': 1125, 'fiction': 1126, 'figure': 1127, 'filmmakers': 1128, 'finish': 1129, 'flick': 1130, 'follow': 1131, 'font': 1132, 'forced': 1133, 'fresh': 1134, 'gay': 1135, 'generally': 1136, 'german': 1137, 'grace': 1138, 'grade': 1139, 'grew': 1140, 'guys': 1141, 'handled': 1142, 'haunted': 1143, 'hear': 1144, 'height': 1145, 'held': 1146, 'hellox': 1147, 'hero': 1148, 'hi': 1149, 'hinglish': 1150, 'homeless': 1151, 'honor': 1152, 'hopeless': 1153, 'horner': 1154, 'https': 1155, 'humanity': 1156, 'ill': 1157, 'impact': 1158, 'impression': 1159, 'indeed': 1160, 'interiors': 1161, 'involved': 1162, 'jav': 1163, 'joke': 1164, 'karloff': 1165, 'keep': 1166, 'killing': 1167, 'known': 1168, 'lady': 1169, 'laugh': 1170, 'leaving': 1171, 'legendary': 1172, 'lifeboat': 1173, 'lines': 1174, 'living': 1175, 'losing': 1176, 'loss': 1177, 'lovely': 1178, 'lovett': 1179, 'mcelwee': 1180, 'meaning': 1181, 'meets': 1182, 'mel': 1183, 'melodrama': 1184, 'mistake': 1185, 'mix': 1186, 'moral': 1187, 'morgan': 1188, 'mostly': 1189, 'moved': 1190, 'mysterious': 1191, 'near': 1192, 'neat': 1193, 'nominated': 1194, 'nor': 1195, 'nt': 1196, 'okay': 1197, 'ones': 1198, 'opinion': 1199, 'paced': 1200, 'perfectly': 1201, 'personally': 1202, 'plan': 1203, 'planet': 1204, 'pleasure': 1205, 'policemen': 1206, 'pop': 1207, 'portrayed': 1208, 'possibly': 1209, 'price': 1210, 'producers': 1211, 'pt': 1212, 'quot': 1213, 'race': 1214, 'rate': 1215, 'rating': 1216, 'rd': 1217, 'realize': 1218, 'realized': 1219, 'recent': 1220, 'released': 1221, 'rescue': 1222, 'reviews': 1223, 'ride': 1224, 'run': 1225, 'safe': 1226, 'saving': 1227, 'saying': 1228, 'says': 1229, 'screenplay': 1230, 'season': 1231, 'seasons': 1232, 'setting': 1233, 'shekhar': 1234, 'singh': 1235, 'slapstick': 1236, 'sleep': 1237, 'slightly': 1238, 'small': 1239, 'smart': 1240, 'smoking': 1241, 'society': 1242, 'solution': 1243, 'span': 1244, 'speak': 1245, 'speech': 1246, 'speed': 1247, 'spent': 1248, 'st': 1249, 'stand': 1250, 'standard': 1251, 'standards': 1252, 'starts': 1253, 'stephanie': 1254, 'stick': 1255, 'stomach': 1256, 'strange': 1257, 'stunts': 1258, 'suffered': 1259, 'suicide': 1260, 'summer': 1261, 'sundance': 1262, 'teachers': 1263, 'theaters': 1264, 'theatre': 1265, 'thumbs': 1266, 'total': 1267, 'totally': 1268, 'touch': 1269, 'touched': 1270, 'touching': 1271, 'track': 1272, 'treasure': 1273, 'twice': 1274, 'u0072': 1275, 'unfortunate': 1276, 'unlike': 1277, 'unsinkable': 1278, 'userid': 1279, 'viewing': 1280, 'vishal': 1281, 'walker': 1282, 'warren': 1283, 'wasting': 1284, 'water': 1285, 'whether': 1286, 'win': 1287, 'wonder': 1288, 'worked': 1289, 'worldss': 1290, 'writing': 1291, 'x28': 1292, 'x29': 1293, 'xmlns': 1294, 'yourself': 1295, 'zane': 1296, '7c': 1297, 'ADD': 1298, 'COMMENT': 1299, 'CONNECTION': 1300, 'DATE': 1301, 'DEPTH': 1302, 'DUAL': 1303, 'FINAL': 1304, 'FREE': 1305, 'FUNCTION': 1306, 'GREATEST': 1307, 'ID': 1308, 'IMPORT': 1309, 'LANGUAGE': 1310, 'LEADING': 1311, 'LEVEL': 1312, 'LINK': 1313, 'LISTEN': 1314, 'NAMESPACE': 1315, 'ORDER': 1316, 'PERIOD': 1317, 'PUBLIC': 1318, 'QUOTE': 1319, 'RETURN': 1320, 'ROW': 1321, 'SELF': 1322, 'SEQUENCE': 1323, 'SIZE': 1324, 'TYPES': 1325, 'WINDOW': 1326, 'XOR': 1327, 'aboard': 1328, 'absurd': 1329, 'abundance': 1330, 'accept': 1331, 'aditya': 1332, 'afraid': 1333, 'age': 1334, 'aged': 1335, 'air': 1336, 'aliens': 1337, 'alive': 1338, 'allows': 1339, 'allowscriptaccess': 1340, 'alternates': 1341, 'america': 1342, 'animation': 1343, 'ann': 1344, 'anybody': 1345, 'apparent': 1346, 'appeal': 1347, 'aroused': 1348, 'arrested': 1349, 'asked': 1350, 'attack': 1351, 'awards': 1352, 'barely': 1353, 'basically': 1354, 'bates': 1355, 'beat': 1356, 'beautifully': 1357, 'bed': 1358, 'behavior': 1359, 'beyond': 1360, 'bill': 1361, 'bin': 1362, 'blah': 1363, 'bloke': 1364, 'blue': 1365, 'board': 1366, 'boat': 1367, 'boyfriend': 1368, 'brando': 1369, 'brief': 1370, 'bubble': 1371, 'bullet': 1372, 'bunch': 1373, 'burn': 1374, 'california': 1375, 'calls': 1376, 'calvert': 1377, 'canadian': 1378, 'cartoon': 1379, 'cause': 1380, 'caused': 1381, 'causes': 1382, 'century': 1383, 'certainly': 1384, 'chaliya': 1385, 'changed': 1386, 'channel': 1387, 'charming': 1388, 'choice': 1389, 'cinema': 1390, 'cinematic': 1391, 'clark': 1392, 'click': 1393, 'cmd': 1394, 'co': 1395, 'collision': 1396, 'comedies': 1397, 'common': 1398, 'completed': 1399, 'computer': 1400, 'concerned': 1401, 'conn': 1402, 'consider': 1403, 'cooper': 1404, 'core': 1405, 'costuming': 1406, 'cover': 1407, 'crack': 1408, 'credit': 1409, 'critics': 1410, 'culkin': 1411, 'dante': 1412, 'deal': 1413, 'decides': 1414, 'desire': 1415, 'destroy': 1416, 'destruction': 1417, 'detail': 1418, 'details': 1419, 'dhoom': 1420, 'dinner': 1421, 'disappointment': 1422, 'discover': 1423, 'discovered': 1424, 'dislike': 1425, 'downright': 1426, 'dress': 1427, 'dressed': 1428, 'due': 1429, 'e': 1430, 'economy': 1431, 'edison': 1432, 'edited': 1433, 'effect': 1434, 'elderly': 1435, 'elephant': 1436, 'ended': 1437, 'engaging': 1438, 'entertained': 1439, 'entirely': 1440, 'episodes': 1441, 'equally': 1442, 'era': 1443, 'erotic': 1444, 'eventually': 1445, 'example': 1446, 'executed': 1447, 'existence': 1448, 'exotic': 1449, 'exploit': 1450, 'exploration': 1451, 'faced': 1452, 'factor': 1453, 'fails': 1454, 'fate': 1455, 'fated': 1456, 'feature': 1457, 'features': 1458, 'fight': 1459, 'figured': 1460, 'finished': 1461, 'fire': 1462, 'fisher': 1463, 'fix': 1464, 'flawless': 1465, 'flaws': 1466, 'flying': 1467, 'focus': 1468, 'follows': 1469, 'frances': 1470, 'friends': 1471, 'furthermore': 1472, 'gabby': 1473, 'gain': 1474, 'gang': 1475, 'gangster': 1476, 'gantry': 1477, 'generation': 1478, 'genres': 1479, 'george': 1480, 'giant': 1481, 'gibney': 1482, 'gloria': 1483, 'gore': 1484, 'gorgeous': 1485, 'greater': 1486, 'gross': 1487, 'hackneyed': 1488, 'hands': 1489, 'hanging': 1490, 'happen': 1491, 'happy': 1492, 'heavy': 1493, 'higher': 1494, 'hilarious': 1495, 'hitchcock': 1496, 'hitting': 1497, 'hockley': 1498, 'holes': 1499, 'hollow': 1500, 'huge': 1501, 'hyde': 1502, 'hype': 1503, 'ideas': 1504, 'ie': 1505, 'imagination': 1506, 'imdb': 1507, 'importance': 1508, 'impress': 1509, 'impressed': 1510, 'included': 1511, 'incredible': 1512, 'inexplicable': 1513, 'information': 1514, 'innocent': 1515, 'inside': 1516, 'inspired': 1517, 'intellectual': 1518, 'intelligence': 1519, 'intense': 1520, 'iq': 1521, 'isabelle': 1522, 'j': 1523, 'jonathan': 1524, 'journey': 1525, 'judge': 1526, 'julie': 1527, 'jumping': 1528, 'kathy': 1529, 'keeps': 1530, 'kick': 1531, 'kiss': 1532, 'kristin': 1533, 'lakhan': 1534, 'lately': 1535, 'laughable': 1536, 'leads': 1537, 'legend': 1538, 'levels': 1539, 'lifeboats': 1540, 'lights': 1541, 'likable': 1542, 'login': 1543, 'loses': 1544, 'loud': 1545, 'lover': 1546, 'lovers': 1547, 'matrix': 1548, 'meaningful': 1549, 'mentioned': 1550, 'message': 1551, 'middle': 1552, 'miller': 1553, 'miss': 1554, 'missing': 1555, 'mom': 1556, 'monster': 1557, 'motion': 1558, 'motor': 1559, 'msb': 1560, 'mummy': 1561, 'myself': 1562, 'narration': 1563, 'narrative': 1564, 'nightmare': 1565, 'nintendo': 1566, 'non': 1567, 'north': 1568, 'note': 1569, 'nowhere': 1570, 'nude': 1571, 'nudie': 1572, 'nudity': 1573, 'obvious': 1574, 'officers': 1575, 'onto': 1576, 'oscars': 1577, 'otherwise': 1578, 'overkill': 1579, 'paint': 1580, 'pandey': 1581, 'parents': 1582, 'particular': 1583, 'particularly': 1584, 'pathetic': 1585, 'paxton': 1586, 'personal': 1587, 'physically': 1588, 'pictures': 1589, 'pink': 1590, 'plenty': 1591, 'poem': 1592, 'political': 1593, 'poorly': 1594, 'portrays': 1595, 'previous': 1596, 'produced': 1597, 'producing': 1598, 'profession': 1599, 'promo': 1600, 'provocative': 1601, 'pure': 1602, 'radio': 1603, 'rarely': 1604, 'realm': 1605, 'recently': 1606, 'red': 1607, 'remembered': 1608, 'renying': 1609, 'resort': 1610, 'revealing': 1611, 'rms': 1612, 'road': 1613, 'rubbish': 1614, 'ruin': 1615, 'ruined': 1616, 'rydell': 1617, 'sandra': 1618, 'sank': 1619, 'scary': 1620, 'scenery': 1621, 'science': 1622, 'scott': 1623, 'secrets': 1624, 'sentimental': 1625, 'seriously': 1626, 'several': 1627, 'sexual': 1628, 'shared': 1629, 'shock': 1630, 'shocking': 1631, 'shouldn': 1632, 'showed': 1633, 'shower': 1634, 'shut': 1635, 'single': 1636, 'sister': 1637, 'situations': 1638, 'slim': 1639, 'sometimes': 1640, 'spending': 1641, 'spiderman': 1642, 'spoiler': 1643, 'squad': 1644, 'stands': 1645, 'starting': 1646, 'status': 1647, 'steamy': 1648, 'steeped': 1649, 'storm': 1650, 'storyline': 1651, 'stuart': 1652, 'stupidity': 1653, 'submit': 1654, 'subtle': 1655, 'sucks': 1656, 'superb': 1657, 'survive': 1658, 'survivors': 1659, 'sweet': 1660, 'syscat': 1661, 'taylor': 1662, 'teach': 1663, 'teenage': 1664, 'themes': 1665, 'therese': 1666, 'thomas': 1667, 'thousand': 1668, 'thousands': 1669, 'thrill': 1670, 'tone': 1671, 'towards': 1672, 'tragedy': 1673, 'transfer': 1674, 'translated': 1675, 'truth': 1676, 'turning': 1677, 'twenty': 1678, 'typical': 1679, 'u0065': 1680, 'u006c': 1681, 'unnecessary': 1682, 'unrealistic': 1683, 'unusual': 1684, 'ups': 1685, 'users': 1686, 'usually': 1687, 'v': 1688, 'victim': 1689, 'viewers': 1690, 'villain': 1691, 'virus': 1692, 'wait': 1693, 'walking': 1694, 'wall': 1695, 'wanted': 1696, 'war': 1697, 'warner': 1698, 'warning': 1699, 'ways': 1700, 'wb': 1701, 'weak': 1702, 'weird': 1703, 'welcome': 1704, 'western': 1705, 'whom': 1706, 'whose': 1707, 'width': 1708, 'willing': 1709, 'winning': 1710, 'wo': 1711, 'wood': 1712, 'words': 1713, 'worn': 1714, 'wrapped': 1715, 'x00003c': 1716, 'x0003c': 1717, 'xu': 1718, 'younger': 1719, 'zhou': 1720, 'zhu': 1721, 'zmed': 1722, 'zombie': 1723, 'zombies': 1724, '™': 1725})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "train_data, valid_data = training_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n",
    "#initialize glove embeddings\n",
    "TEXT.build_vocab(train_data,min_freq=3,vectors = \"glove.6B.100d\")  \n",
    "LABEL.build_vocab(train_data)\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "#print(TEXT.vocab.freqs.most_common(100))  \n",
    "print(TEXT.vocab.stoi)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "\n",
    "#set batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class classifier(nn.Module):\n",
    "    \n",
    "    #define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        \n",
    "        #Constructor\n",
    "        super().__init__()          \n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        #dense layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        #activation function\n",
    "#        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [batch size,sent_length]\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent_len, emb dim]\n",
    "      \n",
    "#packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        #hidden = [batch size, num layers * num directions,hid dim]\n",
    "        #cell = [batch size, num layers * num directions,hid dim]\n",
    "        \n",
    "        #concat the final forward and backward hidden state\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        dense_outputs=self.fc(hidden)\n",
    "\n",
    "        #Final activation function\n",
    "        outputs= dense_outputs#self.act(dense_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'sql': 0, 'xss': 1, 'safe': 2, 'label': 3})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#LABEL.vocab.stoi.pop(\"label\")\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_vocab = len(TEXT.vocab)\n",
    "embedding_dim = 100\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = len(LABEL.vocab.stoi)\n",
    "num_layers = 2\n",
    "bidirection = True\n",
    "dropout = 0.2\n",
    "\n",
    "#instantiate the model\n",
    "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \n",
    "                   bidirectional = True, dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier(\n",
      "  (embedding): Embedding(1726, 100)\n",
      "  (lstm): LSTM(100, 32, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "The model has 232,252 trainable parameters\n",
      "torch.Size([1726, 100])\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "\n",
    "#No. of trianable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "#Initialize the pretrained embedding\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_pred = torch.round(preds)\n",
    "    _,pred_label = torch.max(rounded_pred, dim = 1)\n",
    "    correct = (pred_label == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "    \n",
    "#push to cuda if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    #initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    #set the model in training phase\n",
    "    model.train()  \n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        #resets the gradients after every batch\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        #retrieve text and no. of words\n",
    "        text, text_lengths = batch.text   \n",
    "        \n",
    "        #convert to 1D tensor\n",
    "        predictions = model(text, text_lengths).squeeze()  \n",
    "        \n",
    "        #compute the loss\n",
    "        y_tensor = torch.tensor(batch.label, dtype=torch.long, device=device)\n",
    "        loss = criterion(predictions, y_tensor)        \n",
    "        \n",
    "        #compute the binary accuracy\n",
    "        acc = binary_accuracy(predictions, batch.label)   \n",
    "        \n",
    "        #backpropage the loss and compute the gradients\n",
    "        loss.backward()       \n",
    "        \n",
    "        #update the weights\n",
    "        optimizer.step()      \n",
    "        \n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item()  \n",
    "        epoch_acc += acc.item()    \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    #deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    #deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "        \n",
    "            #retrieve text and no. of words\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            #convert to 1d tensor\n",
    "            predictions = model(text, text_lengths).squeeze()\n",
    "            \n",
    "            #compute loss and accuracy\n",
    "            y_tensor = torch.tensor(batch.label, dtype=torch.long, device=device)\n",
    "            loss = criterion(predictions, y_tensor)      \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.334 | Train Acc: 0.10%\n",
      "\t Val. Loss: 1.238 |  Val. Acc: 0.46%\n",
      "\tTrain Loss: 1.144 | Train Acc: 26.17%\n",
      "\t Val. Loss: 1.030 |  Val. Acc: 74.11%\n",
      "\tTrain Loss: 0.860 | Train Acc: 60.91%\n",
      "\t Val. Loss: 0.677 |  Val. Acc: 79.35%\n",
      "\tTrain Loss: 0.460 | Train Acc: 88.57%\n",
      "\t Val. Loss: 0.312 |  Val. Acc: 93.74%\n",
      "\tTrain Loss: 0.225 | Train Acc: 95.90%\n",
      "\t Val. Loss: 0.190 |  Val. Acc: 95.06%\n",
      "\tTrain Loss: 0.143 | Train Acc: 96.39%\n",
      "\t Val. Loss: 0.129 |  Val. Acc: 95.51%\n",
      "\tTrain Loss: 0.111 | Train Acc: 96.19%\n",
      "\t Val. Loss: 0.152 |  Val. Acc: 94.85%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 7\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    #train the model\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    #evaluate the model\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "# lets save the model\n",
    "def save_model():\n",
    "    models_path = \"saved_weights\"\n",
    "    md_val_acc = \"%.2f\"%(valid_acc*100)\n",
    "    model_name = \"Acc \"+md_val_acc+\".pt\"\n",
    "    full_path = os.path.join(models_path, model_name)\n",
    "    torch.save(model.state_dict(),full_path)\n",
    "    print(\"SAVED\\n\",model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED\n",
      " Acc 94.85.pt\n"
     ]
    }
   ],
   "source": [
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list = LABEL.vocab.stoi.pop(\"label\")\n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict(model,sentence):\n",
    "    pred_2_lbl = {1:'xss',0:'sql',2:\"s\"}\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sql_tokenizer(sentence))] # tokenize the sentence\n",
    "    print(tokenized)\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized] # convert to integer sequence\n",
    "    print(indexed)\n",
    "    length = [len(indexed)] #compute no. of words\n",
    "    tensor = torch.LongTensor(indexed).to(device) # convert to tensor\n",
    "    tensor = tensor.unsqueeze(1).T\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = model(tensor,length_tensor)\n",
    "    pred_lbl = np.argmax(prediction.detach().numpy())\n",
    "    print('\\n')\n",
    "    print('predicted threat type:',pred_2_lbl[pred_lbl])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'SELECT', 'STAR', 'FROM', 'items', '\\n', 'where', 'OWNER', 'EQ', 'SQUT', 'wiley', 'SQUT', 'AND', 'itemname', 'EQ', 'SQUT', 'NAME', 'SQUT', 'OR', 'SQUT', 'A', 'SQUT', 'EQ', 'SQUT', 'A', 'SQUT', 'SMCLN']\n",
      "[0, 64, 48, 45, 0, 0, 0, 0, 22, 12, 0, 12, 9, 0, 22, 12, 379, 12, 49, 12, 7, 12, 22, 12, 7, 12, 8]\n",
      "\n",
      "\n",
      "predicted threat type: sql\n"
     ]
    }
   ],
   "source": [
    "pred = predict(model,\"\"\" SELECT * FROM items\n",
    "WHERE owner = 'wiley'\n",
    "AND itemname = 'name' OR 'a'='a'; \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('sql', 0), ('xss', 1), ('safe', 2), ('label', 3)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safe Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'm', 'good']\n",
      "[18, 837, 77]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-91d4eb8afc27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'im good'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-38f9b7fc042c>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(model, sentence)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mpred_lbl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predicted threat type:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred_2_lbl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpred_lbl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "predict(model,'im good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
